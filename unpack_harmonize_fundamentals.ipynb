{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "unpack_harmonize_fundamentals.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPlaHLV9kWUpezVQQ7852Zo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/juanlazarde/financial_scanner/blob/master/unpack_harmonize_fundamentals.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DhmalI3Wmc6J"
      },
      "source": [
        "# Léete esto primero\r\n",
        "Escrito para mi pana Ale y puesto en Notebook, bello para que no se queje. Léete la vaina.\r\n",
        "\r\n",
        "# Latest unpacking of fundamentals code.\r\n",
        "\r\n",
        "**Motivation**: Create dictionary that will replace and harmonize column labels for fundamentals tables, and deduplicate data from same source. We will run this script for every new source of data, i.e. Yahoo, TDA, Alpaca, etc. It is also a pretty good code to re-use for Google Colab.\r\n",
        "\r\n",
        "**Expected Outcome**: CSV/Spreadhseet file with original field names, and equivalent.\r\n",
        "\r\n",
        "**Consideration**: The final mega table with all data from all sources will be filtered with these documents; one per source. If the name does not have an equivalent in this file, it means that it will nor be registered in the database. This way, we will only keep the data from the sources we want.\r\n",
        "\r\n",
        "Google Colab handles Google Spread, otherwise use CSV.\r\n",
        "\r\n",
        "## Instructions\r\n",
        "* Code works in Colab, not tested in Jupyter. Won't work on IDE.\r\n",
        "* Code is not for production, so not PEP8.\r\n",
        "* No atomized functions for easier read.\r\n",
        "* Edit the settings below.\r\n",
        "* Two things will happen after loading the data, 1) it will stop and show a\r\n",
        "  table if there's duplicated data, 2) it will output a dataframe with the\r\n",
        "  modifed labels. But, we're interested in the dictionary and the deduplication.\r\n",
        "\r\n",
        "### Standard for names\r\n",
        "* all lower caps\r\n",
        "* snake_case (perhaps we can parse by '_' and Capitalize next letter for user readability)\r\n",
        "* No $ymbols"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCS3pU2kmdiw"
      },
      "source": [
        "# settings\r\n",
        "google_colab = False  # using Google Colab tools\r\n",
        "get_data = True  # download fresh data\r\n",
        "export_df = True  # export the freshly downloaded data.columns\r\n",
        "to_csv = True # save it to a csv or if False to Google Sheet\r\n",
        "import_df = True  # import the data.columns modified by user\r\n",
        "import_csv = True  # get it from csv or if False from Google Sheet\r\n",
        "apply_to_df = True  # apply new labels to freshly downloaded data\r\n",
        "#file = \"/content/drive/MyDrive/columns_in_yahoo\"\r\n",
        "file = \"columns_in_yahoo\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2-iynIUF09Qy"
      },
      "source": [
        "Find your `file` here:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DUi27el_1GRE"
      },
      "source": [
        "# current directory\r\n",
        "%pwd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DOnluqA41rE9"
      },
      "source": [
        "# directories and files here\r\n",
        "%ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZAP6Uwsa1rMA"
      },
      "source": [
        "# change directory\r\n",
        "# %cd \"c:/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mO0LC-p1C95"
      },
      "source": [
        "Loading some modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qUQomKiZodjx"
      },
      "source": [
        "# load modules\r\n",
        "try:\r\n",
        "  import yahooquery as yq\r\n",
        "except:\r\n",
        "  %pip install yahooquery\r\n",
        "  import yahooquery as yq\r\n",
        "import pandas as pd\r\n",
        "import sys"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mx3C6P_ForzZ"
      },
      "source": [
        "Custom code below show() and stop(), I've found very useful for Notebook breakpoint and debugging. It shows ALL DataFrame rows and columns. Stop uses ```raise SystemExit``` which is the same as ```sys.exit(0)```, they both generate a SystemExit exception that looks like an error, it is not. Usage:\r\n",
        "```\r\n",
        "# stop(df)\r\n",
        "\r\n",
        "# stop(True, df)   # this will stop and reset all variables\r\n",
        "\r\n",
        "# stop(\"Stopping here\")\r\n",
        "\r\n",
        "# try:\r\n",
        "#   raise \r\n",
        "# except Exception as e:\r\n",
        "#  stop(f'Error here {e})\r\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OP28nxMeooKo"
      },
      "source": [
        "# -------- CUSTOM TOOLS FOR DEBUGGING -------\r\n",
        "def show(*args):\r\n",
        "  # pandas display options\r\n",
        "  pd.set_option('display.max_rows', None)\r\n",
        "  pd.set_option('display.max_columns', None)\r\n",
        "  display(*args)\r\n",
        "  pd.reset_option('display.max_rows')\r\n",
        "  pd.reset_option('display.max_columns')\r\n",
        "\r\n",
        "def stop(*args):\r\n",
        "  \"\"\"shows and stops, if first argument is True, it resets all variables\"\"\"\r\n",
        "  show(*args)\r\n",
        "  if isinstance(args[0], bool) and args[0]:\r\n",
        "    print(\"****** Variables Reset *******\")\r\n",
        "    %reset -f # to skip prompt\r\n",
        "  raise SystemExit\r\n",
        "\r\n",
        "show(\"show() works\")\r\n",
        "for i in range(20):\r\n",
        "  stop(\"stop() works. Yes, the exception is on purpose\")\r\n",
        "print(\"stop() doesn't work\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0uPrwyN1q5pC"
      },
      "source": [
        "# Download Function\r\n",
        "This is the near-final code for downloading and unpacking Yahoo, it will also unpack TDA and any other dictionary. It's a fancy json_normalize. Document any changes.\r\n",
        "\r\n",
        "* Near-final beacuse it will have to include the deduplication code, which is unique to each database. This deduplication code is included in the Research area.\r\n",
        "* This code has temporary lines demarked with # DEBUGGING, these are to find the source of the duplicated data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Q2uq3rKqCcY"
      },
      "source": [
        "# --------  DOWNLOAD AND UNPACK ----------------\r\n",
        "def download_unpack_yahoo():\r\n",
        "  # symbols and chunks to download\r\n",
        "  symbols = ['AAPL', 'FB', 'AMZN', 'TSLA', 'QQQ', 'BSX']\r\n",
        "  chunk_size = len(symbols)  # no chunking\r\n",
        "\r\n",
        "  # download stocks data in chunks and concatenate to large table\r\n",
        "  df = pd.DataFrame()\r\n",
        "  for i in range(0, len(symbols), chunk_size):\r\n",
        "    symbol_chunk = symbols[i:i + chunk_size]\r\n",
        "    tickers = yq.Ticker(symbols=symbol_chunk, asynchronous=True).all_modules\r\n",
        "    chunk_df = pd.DataFrame(tickers).T\r\n",
        "    df = pd.concat([df, chunk_df])\r\n",
        "\r\n",
        "  # unpack dictionaries and concatenate to table\r\n",
        "  # packages = ['summaryProfile', 'summaryDetail']  # by name\r\n",
        "  packages = df.columns  # all\r\n",
        "\r\n",
        "  # adding prefix to avoid duplicated label errors unpacking\r\n",
        "  df = df.add_prefix(\"_\")\r\n",
        "  packages = [\"_\" + i for i in packages]\r\n",
        "\r\n",
        "  df = pd.concat([pd.DataFrame(None, index=['package']), df])  # DEBUGGING\r\n",
        "  for package in packages:\r\n",
        "    on_hand = df.loc[df[package].notnull(), package]\r\n",
        "    unpack_df = pd.json_normalize(on_hand).set_index(on_hand.index)\r\n",
        "    unpack_df.loc['package'] = package[1:]  # DEBUGGING\r\n",
        "    df.drop(package, axis=1, inplace=True)\r\n",
        "    df = pd.concat([df, unpack_df], axis=1)\r\n",
        "\r\n",
        "  # cleanup code\r\n",
        "  # when done, insert the 'deal with duplicates' section here\r\n",
        "  df.rename_axis('symbol', inplace=True)\r\n",
        "  return df\r\n",
        "\r\n",
        "yf = download_unpack_yahoo()\r\n",
        "yf.T"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xSseoSw2sfDY"
      },
      "source": [
        "# Research Area\r\n",
        "Here we will find duplicated fields and we'll be able to download the list of fields in a CSV/GoogleSheet to be edited separately, then upload it, and then apply the corresponding dictionary to the DataFrame for testing. We'll need the dictionary. We may keep it in the dataset folder of the financial scanner.\r\n",
        "\r\n",
        "* Using Google Colab Research, so you'll see many references to it. Disable it in the settings above ```google_colab = False```\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4pm7vrKesdzw"
      },
      "source": [
        "if google_colab:\r\n",
        "    from google.colab import auth, drive, files\r\n",
        "    from oauth2client.client import GoogleCredentials\r\n",
        "    import gspread\r\n",
        "    import io\r\n",
        "    auth.authenticate_user()\r\n",
        "    drive.mount(\"/content/drive\")\r\n",
        "    gc = gspread.authorize(GoogleCredentials.get_application_default())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYo1QnvUuncm"
      },
      "source": [
        "Populate DataFrame with the latest download, or if Google Colab you can upload a CSV with the DataFrame.\r\n",
        "This data changes every time, becuase we'll download with Async"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LgsLt7OwulPQ"
      },
      "source": [
        "# download or use file for full DataFrame\r\n",
        "if get_data:\r\n",
        "  try:\r\n",
        "    yf = download_unpack_yahoo()\r\n",
        "  except:\r\n",
        "    if google_colab:\r\n",
        "      uploaded = files.upload()\r\n",
        "      yf = pd.read_csv(io.BytesIO(uploaded[list(uploaded)[0]]))\r\n",
        "      yf.drop(columns=['Unnamed: 0'], inplace=True)\r\n",
        "    else:\r\n",
        "      stop(\"Error getting data\")\r\n",
        "yf.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZLk08LFHvETs"
      },
      "source": [
        "## Deduplication\r\n",
        "Here is the code to find the duplicated fields. We need to either rename or delete these. This happens because two sources have the same field, i.e. SummaryProfile has quoteType and assetProfile has quoteType.\r\n",
        "* List ```erase = ['maxAge', 'symbol']``` is a working example for YahooQuery\r\n",
        "* The output of this section has to be a list or custom code that we can embed in the final code.\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r7eYsENBu9wu"
      },
      "source": [
        "# deal with duplicates\r\n",
        "erase = ['maxAge', 'symbol']\r\n",
        "try:\r\n",
        "  yf.drop(erase, axis='columns', inplace=True, errors='ignore')\r\n",
        "except AttributeError as e:\r\n",
        "  stop(f\"Dataframe may be empty. {e}\")\r\n",
        "df = yf.loc[:,yf.columns.duplicated(keep=False)].T.sort_index()\r\n",
        "if not df.empty:\r\n",
        "  print(\"Data has duplicate index, deal with this first.\")\r\n",
        "  stop(df.iloc[:, 0:5])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sbs6uf8Mw23B"
      },
      "source": [
        "Here we can export the DataFrame columns for easier edit on CSV or Google Sheet.\r\n",
        "* Advantage of Google Sheet is that we can share the sheet and we can both edit. Whether we use the feature or not, this time around."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "36cGwSUTv5LV"
      },
      "source": [
        "if export_df:\r\n",
        "  # outgoing format\r\n",
        "  df = pd.DataFrame([yf.columns.unique(), ''], index=['current', 'new']).T\r\n",
        "  df = df[['new', 'current']]\r\n",
        "  try:\r\n",
        "    if to_csv:\r\n",
        "      # save to csv\r\n",
        "      df.to_csv(file+\".csv\", index=False, header=True)\r\n",
        "      print(f'Saved CSV file {file}')\r\n",
        "    elif google_colab:\r\n",
        "      # save to Google Sheet\r\n",
        "      file = file.split(\"/\")[-1].split(\".\")[0]\r\n",
        "      gc.create(file)\r\n",
        "      ws = gc.open(file).get_worksheet(0)\r\n",
        "      from gspread_dataframe import set_with_dataframe\r\n",
        "      set_with_dataframe(ws, df)\r\n",
        "      print(f'Saved Google Sheets file {file}')\r\n",
        "    else:\r\n",
        "      raise\r\n",
        "  except FileNotFoundError as e:\r\n",
        "      stop(f\"Couldn't export DataFrame. {e}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SJahJlk0xOcv"
      },
      "source": [
        "Here we can import the CSV or Google Sheet after being edited."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eiggm7oexJyW"
      },
      "source": [
        "if import_df:\r\n",
        "  try:\r\n",
        "    if import_csv:\r\n",
        "        df = pd.read_csv(file+\".csv\")\r\n",
        "    elif google_colab:\r\n",
        "      # get from Google Drive\r\n",
        "      file = file.split(\"/\")[-1].split(\".\")[0]\r\n",
        "      rows = gc.open(file).get_worksheet(0).get_all_values()\r\n",
        "      df = pd.DataFrame.from_records(rows)\r\n",
        "      # format similar to read_csv\r\n",
        "      # df.set_index(0, drop=True, inplace=True)\r\n",
        "      df = df.rename(columns=df.iloc[0]).drop(df.index[0])\r\n",
        "    else:\r\n",
        "      raise\r\n",
        "  except FileNotFoundError as e:\r\n",
        "    stop(f\"Couldn't import the DataFrame. {e}\")\r\n",
        "  show(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xw_lcJvwxpJo"
      },
      "source": [
        "# Final check\r\n",
        "Here we'll see how the final DataFrame look like.\r\n",
        "* Remember that here we rename the labels.\r\n",
        "* Delete columns with NaN in 'current' definition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CjdfUxvuxn6u"
      },
      "source": [
        "if apply_to_df and import_df:\r\n",
        "  # prep for dictionary\r\n",
        "  df1 = df.loc[(df['new'].notnull()) & (df['new'] != '')]\r\n",
        "  dt = df1.to_dict(orient='records')\r\n",
        "  dt = {i['current']: i['new'] for i in dt}\r\n",
        "\r\n",
        "  # renames the lables using the dictionary\r\n",
        "  yf.rename(columns=dt, inplace=True)\r\n",
        "\r\n",
        "  # deletes columns not in the dictionary, because were deemed unimportant\r\n",
        "  drop_me = df.loc[~df['new'].str.strip().astype(bool)]['current'].to_list()\r\n",
        "  yf.drop(columns=dropped_columns, inplace=True)\r\n",
        "\r\n",
        "  print(\"\\nChanges you made:\\n\"\r\n",
        "        \"-----------------\")\r\n",
        "  show(dt)\r\n",
        "  print(\"\\nFind your changes here:\\n\"\r\n",
        "        \"-----------------------\")\r\n",
        "  show(yf.T.iloc[:,1])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}